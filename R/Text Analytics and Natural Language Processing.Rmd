---
title: "Text Analytics and Natural Language Processing"
author: "Timon-Laurin Krämer"
date: "12/2/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Part 1: Motivation

Ultimate Objective - Why?

- Extract meaningful business insights out of unstructured data

What is unstructured data?
- Unstructured data is any data that is unorganized
- You might see patterns in nature, but everything is unstructured. 

Zitat (Jack Sparrow): All data that is now structured was unstructured before it was structures 

Examples: 
- Tweets 
- Surveys
- Images
- Audio
- Conversations
- Whitepaper


Background

-  80% of the data is unstructured 

- State Hypothesis
- Does it get better or worse in the future?
- Is there a sizable market?
- Impact if we could solve this problem?


### Main Objectives

- Use Natural Language Processing to extract business insight or solve a specific problem. 

What is NLP? 

NLP stands for Natural Language Processing

What benefits could NLP bring to your business?

- Understand & Monitor the social channels more effectively
  - What makes customers happy / unhappy
  - Customer Preferences
  - Improve customer service
  - Find out Painpoints of a business. 
- Find insigths in business documents
  - lots of insigths are hidden in word / pdf files
- Analyze areas of improvements more efficiently (Customer Feedback is mostly in speech or text form)
- Get a sense for the sentiment 

- Improve 


### Problem Statement

- Noisy data
- Sentiment very subjective (this is why multiple libraries should be compared)

### What could be a positive outcome?


## Part 2: Method

Strategic Objectives - What & How?


Frequency-Analysis (Descriptive)
- Which tokens are the most frequent?
- Which tokens have the most Business Value?
- Which tokens can be removed?

Analyze the Sentiment (Descriptive)
- Different libraries (afinn, bing and nrc)
- Is the overall sentiment positive or negative?
- How does the sentiment change over time

Relationship between words (Descriptive)
- Tokenization for ngrams 

Predict a topic for a given document (Predictive)
- Latent Dirichlet Allocation





What key ressources do we acquire?

Book data by Jane Austen 

## Part 3: Mechanics

### Tokenization

Tokenization for splitting the text into Tidy text format (one-token-per-document-per-row). This lets us use the popular suite of tidy tools such as dplyr, tidyr, and ggplot2 to explore and visualize text data. 

```{r}
# Import the libraries
library(dplyr)
library(janeaustenr)
library(tidytext)

# The first step is to unnest the data by Tokenization
# Make sure that the column in the dataframe you want to tokenize is called text 
book_words <- austen_books() %>%
  unnest_tokens(word, text)
  
# Count the frequency of the words
book_words <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE)

print(head(book_words))

```


The typical stopwords ["the", "to", "and"] have to be filtered. 

```{r}
library(stringr)
# Convert data into tidy format and filter out stopwords
data(stop_words)

# Antijoin the stopwords
tidy_books <- austen_books() %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) 

# Custom stop words can be added to the lexicon like that
custom_stop_words <- bind_rows(tibble(word = c("miss"),  
                                      lexicon = c("custom")), 
                               stop_words)

# Count filtered words per book 
tidy_books_count <- austen_books() %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  group_by(book) %>% 
  count(book, word, sort = TRUE)


```

It it is possible though that some of these words actually have business value. This is why another approach called tfidf can be used as well. 

The statistic tf-idf is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites.


## tfidf

```{r}

# Get frequency of words in all books
book_words <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE)

# Sum up all the words per book 
total_words <- book_words %>% 
  group_by(book) %>% 
  summarize(total = sum(n))

# Join 
book_words <- left_join(book_words, total_words)

```

The following shows the distribution of n/total for each novel. 

```{r}

library(ggplot2)

ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y")

```

## Sentiment Analysis: Summary Scores

There are different sentiment libraries available. Afinn lexicon uses scores ranging from -5 to 5. Bing classifies the word either as positive or negative. And the nrc lexicon gives the meaning of the word a 'flavor'. This allows to count the appearance of positive or negative words, according to the specified reference lexicon.

- Afinn (-5 to 5)
- Bing (Positive | Negative)
- NRC (Joy, Anger, Sadness, Fear etc.)

However, for more meaningful business insights the semantics play a bigger role. For example, the words “happy” and “like” will be counted as positive, even in a sentence like “I’m not happy and I don’t like it!”


```{r}
# Lets do a sentiment analysis of the Mansfield Park Book
mansfield <- book_words %>%
  filter(book == "Mansfield Park")

# Add the Sentiment Score of the library (-5 to 5) and summarize score
afinn <- mansfield %>%
  inner_join(get_sentiments("afinn"))%>%
  summarise(sentiment=sum(value)) %>%
  mutate(method="AFINN")

## Do the same for the Bing and NRC library
bing_and_nrc <- bind_rows(
  mansfield%>%
    inner_join(get_sentiments("bing"))%>%
    mutate(method = "Bing et al."),
  india %>%
    inner_join(get_sentiments("nrc") %>%
                 filter(sentiment %in% c("positive", "negative"))) %>%
    mutate(method = "NRC")) %>%
  count(method,  sentiment) %>%
  spread(sentiment, n, fill=0) %>%
  mutate(sentiment = positive-negative)

# Plot the summary scores of the different sentiment libraries

bind_rows(afinn, bing_and_nrc) %>%
  ggplot(aes(method, sentiment, fill=method))+
  geom_col(show.legend=FALSE)+
  facet_wrap(~method, ncol =1, scales= "free_y")

```

We can see the total sentiment scores but we might be more interested in how the story unfolds. The following code introduces new columns to keep track of the line and chapter each word comes from. 


```{r}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

```

Use the name word as output column from unnest to simplify working with the sentiments lexicons. We can also filter for specific sentiments

```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)

```

## Sentiment Analysis: Bing sentiment over time

```{r}
library(tidyr)

jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

```{r}
# Plot the results
library(ggplot2)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

## Comparing the three different dictonaries over time

```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  pride_prejudice %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```


## Contribution to Sentiment

We can also see how the different words contribute to the sentiment. 

```{r}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts 

```
## Visualize the contribution to sentiment (bing lexicon)

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```


# Relationships between words

Until now, we only looked at single words. However, to extract meaningful business insigths, the semantic structure of the text plays a bigger role. The unnest_token function can still be used but has to include the argument token = "ngrams", so that the text gets tokenized by pairs.  


```{r}
# Create bigrams
austen_bigrams <- austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# Count frequency of bigrams
austen_bigrams %>%
  count(bigram, sort = TRUE)
```

We can see again that the typical stopwords show up the most frequently.  Pairs of consecutive words might capture structure that isn’t present when one is just counting single words, and may provide context that makes tokens more understandable. Therefore, we have to split the bigram into single words and remove stopwords. 

```{r}
# Split the bigram into word1 and word2
bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# Filter out the stopwords
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 50) %>%
  graph_from_data_frame()

# Plot the results using the ggraph function
library(ggraph)
set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

The Chart is quite messy so we can add a few stylistic options to make the graph look better. 

```{r}
set.seed(2020)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

To make life easier moving forward, we can place the code to unfilter the stopwords and count the bigrams in a function. We create another function to visualize the graph. 

```{r}
library(dplyr)
library(tidyr)
library(tidytext)
library(ggplot2)
library(igraph)
library(ggraph)

count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}
```

As soon as we defined the functions we can do the whole operation more efficiently. 

```{r}
library(stringr)

# Count the bigrams for a specific book
austen_bigrams <- austen_books() %>%
  filter(`book`=='Sense & Sensibility') %>%
  count_bigrams()

# Filter for rare combinations and also exclude digits
austen_bigrams %>%
  filter(n > 10,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
  visualize_bigrams()

```




## Frequencies using tfidf

What is tf?

Term Frequency

What is idf?

Inverse Document Frequency

idf tells us how unique this token is
Log_e(Count_of_all_docs / Count_of_docs_with_token)

After we removed the stop words, we can unite the two words again and look at the frequency of the term and the uniqueness of the term. 

```{r}

# Unite words 
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# tfidf
bigram_tf_idf <- bigrams_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf
```

## Visualize the network of bigram

The graph view allows us to visualize the relationships among words. Therefore, we need to provide the following input:

- from: the node an edge is coming from
- to: the node an edge is going towards
- weight: A numeric value associated with each edge

```{r}
library(igraph)

# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 20) %>%
  graph_from_data_frame()

bigram_graph
```

This igraph object can be displayed using the ggraph function. 

```{r}
library(ggraph)
set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```




So far the unstructured data was stored in the tidy format. Lets have a look at the Document-Term Matrix.  They are especially useful for classification models. 

- Each row represents one document
- Each column represents one term (word)
- Each cell in the table contains the frequency of that term per document 

```{r}

# The data can also be converted into a Document-Term Matrix (DTM)
book_words_dtm <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE) %>%
  cast_dtm(book, word, n)

print(head(book_words_dtm))

```

Sparsity counts the cells with a frequency of 0 and divides them by the total number of cells in the table (rows*columns). 

The higher the sparsity, the easier it is to classify based on terms.

```{r}
# Convert from dtm to tidy format
book_words_tidy_format <- tidy(book_words_dtm)


```

# Wordcloud

```{r}
library(wordcloud)

tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```


## Part 4: Message

Key Findings?

Value for Company?

Value for Customer?

Next Steps?

